#!/usr/bin/env python3
"""
OpenCode Doc Steward Performance Test Suite
============================================

Evaluates OpenCode's effectiveness as doc steward across:
- Reliability (success rates, error handling)
- Speed (latency, throughput)
- Accuracy (correct changes, no regressions)
- Resource usage (tokens, API calls, cost)

Usage:
    python scripts/opencode_performance_tests.py --all
    python scripts/opencode_performance_tests.py --reliability
    python scripts/opencode_performance_tests.py --speed
"""

import os
import sys
import json
import time
import argparse
import subprocess
import tempfile
import shutil
import requests
import threading
import concurrent.futures
from datetime import datetime
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Any, Optional
from pathlib import Path

# Reuse CI runner logic
import opencode_ci_runner as runner_lib

# ==============================================================================
# CONFIGURATION
# ==============================================================================

EVIDENCE_DIR = "artifacts/evidence/opencode_performance"
TEST_FIXTURES_DIR = "docs/test"
RUNNER_SCRIPT = "scripts/opencode_ci_runner.py"
DEFAULT_PORT = 62587

# Import canonical defaults from single source of truth
try:
    from runtime.agents.models import (
        resolve_model_auto,
        load_model_config
    )
    
    # Resolve default model dynamically
    _config = load_model_config()
    DEFAULT_MODEL, _, _ = resolve_model_auto("steward", _config)
    
except ImportError:
    print("CRITICAL: Failed to import runtime.agents.models")
    sys.exit(1)

# ==============================================================================
# DATA CLASSES
# ==============================================================================

@dataclass
class TestResult:
    test_id: str
    name: str
    category: str
    status: str  # PASS, FAIL, ERROR, SKIP
    latency_ms: float = 0
    details: str = ""
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class MetricsSummary:
    reliability: Dict[str, float] = field(default_factory=dict)
    speed: Dict[str, float] = field(default_factory=dict)
    accuracy: Dict[str, float] = field(default_factory=dict)
    resource: Dict[str, float] = field(default_factory=dict)


# ==============================================================================
# METRICS COLLECTOR
# ==============================================================================

class MetricsCollector:
    """Collects and aggregates performance metrics (Thread-Safe)."""
    
    def __init__(self):
        self.results: List[TestResult] = []
        self.latencies: List[float] = []
        self.token_counts: List[Dict[str, int]] = []
        self.api_call_counts: List[int] = []
        self._timer_start: Optional[float] = None
        self._lock = threading.Lock()
        # Per-thread timers for parallel execution support
        self._thread_timers = threading.local()
        
    def start_timer(self):
        # Use thread-local storage for timers to support parallel execution
        self._thread_timers.start_time = time.time()
        
    def stop_timer(self) -> float:
        if not hasattr(self._thread_timers, 'start_time') or self._thread_timers.start_time is None:
            return 0
        elapsed_ms = (time.time() - self._thread_timers.start_time) * 1000
        self._thread_timers.start_time = None
        
        with self._lock:
            self.latencies.append(elapsed_ms)
        return elapsed_ms
    
    def record_result(self, result: TestResult):
        with self._lock:
            self.results.append(result)
        
    def record_tokens(self, input_tokens: int, output_tokens: int):
        with self._lock:
            self.token_counts.append({
                "input": input_tokens,
                "output": output_tokens,
                "total": input_tokens + output_tokens
            })
        
    def record_api_calls(self, count: int):
        with self._lock:
            self.api_call_counts.append(count)
        
    def calculate_summary(self) -> MetricsSummary:
        with self._lock:
            summary = MetricsSummary()
            
            # Reliability metrics
            reliability_results = [r for r in self.results if r.category == "reliability"]
            if reliability_results:
                passed = sum(1 for r in reliability_results if r.status == "PASS")
                summary.reliability["total_tests"] = len(reliability_results)
                summary.reliability["passed"] = passed
                summary.reliability["success_rate"] = passed / len(reliability_results)
            
            # Speed metrics
            if self.latencies:
                sorted_lat = sorted(self.latencies)
                summary.speed["avg_latency_ms"] = sum(self.latencies) / len(self.latencies)
                summary.speed["min_latency_ms"] = min(self.latencies)
                summary.speed["max_latency_ms"] = max(self.latencies)
                summary.speed["p50_latency_ms"] = sorted_lat[len(sorted_lat) // 2]
                summary.speed["p95_latency_ms"] = sorted_lat[int(len(sorted_lat) * 0.95)] if len(sorted_lat) > 1 else sorted_lat[0]
                # Calculate throughput (edits per minute)
                if sum(self.latencies) > 0:
                    total_time_min = sum(self.latencies) / 60000
                    summary.speed["throughput_per_min"] = len(self.latencies) / total_time_min if total_time_min > 0 else 0
            
            # Resource metrics
            if self.token_counts:
                summary.resource["avg_input_tokens"] = sum(t["input"] for t in self.token_counts) / len(self.token_counts)
                summary.resource["avg_output_tokens"] = sum(t["output"] for t in self.token_counts) / len(self.token_counts)
                summary.resource["avg_total_tokens"] = sum(t["total"] for t in self.token_counts) / len(self.token_counts)
            
            if self.api_call_counts:
                summary.resource["avg_api_calls"] = sum(self.api_call_counts) / len(self.api_call_counts)
                
            return summary


# ==============================================================================
# LOGGING
# ==============================================================================
# Thread lock for logging
log_lock = threading.Lock()

class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    BLUE = '\033[94m'
    YELLOW = '\033[93m'
    RESET = '\033[0m'


def log(msg: str, level: str = "info"):
    timestamp = datetime.now().strftime("%H:%M:%S")
    color = Colors.RESET
    if level == "pass": color = Colors.GREEN
    elif level == "fail": color = Colors.RED
    elif level == "info": color = Colors.BLUE
    elif level == "warn": color = Colors.YELLOW
    
    with log_lock:
        print(f"{color}[{timestamp}] {msg}{Colors.RESET}")


# ==============================================================================
# TEST RUNNER
# ==============================================================================

class PerformanceTestRunner:
    """Runs performance tests against OpenCode doc steward."""
    
    def __init__(self, port: int, model: str, repo_root: str, quick: bool = False, warm: bool = False, workers: int = 1):
        self.port = port
        self.model = model
        self.repo_root = repo_root
        self.metrics = MetricsCollector()
        self.quick = quick
        self.warm = warm
        self.workers = workers # Concurrency level
        self.base_url = f"http://127.0.0.1:{port}"
        
        # Warm mode state
        self.server_process = None
        self.config_dir = None
        self.api_key = None
        
    def start_server(self):
        """Start persistent server for warm mode."""
        if not self.warm: return
        
        log("Starting persistent server for Warm Mode...", "info")
        self.api_key = runner_lib.load_steward_key()
        self.config_dir = runner_lib.create_isolated_config(self.api_key, self.model)
        self.server_process = runner_lib.start_ephemeral_server(self.port, self.config_dir, self.api_key)
        
        if not runner_lib.wait_for_server(self.base_url, timeout=30):
            log("Failed to start server in Warm Mode", "warn")
            self.stop_server()
            raise RuntimeError("Warm mode server failed to start")
            
        log("Warm Mode server ready", "pass")
        
    def stop_server(self):
        """Stop persistent server."""
        if self.server_process:
            log("Stopping persistent server...", "info")
            runner_lib.stop_ephemeral_server(self.server_process)
            runner_lib.cleanup_isolated_config(self.config_dir)
            self.server_process = None
            
    def make_task_json(self, files: List[str], action: str, instruction: str) -> str:
        return json.dumps({"files": files, "action": action, "instruction": instruction})
    
    def estimate_tokens(self, text: str) -> int:
        """Estimate tokens (char/4 heuristic)."""
        return len(text) // 4

    def run_mission_http(self, task_json: str) -> bool:
        """Run mission via direct HTTP calls (Warm Mode) with extended timeout."""
        try:
            task = json.loads(task_json)
            instruction = task.get("instruction", "")
            files = task.get("files", [])
            
            # Estimate Input Tokens
            total_input_text = instruction
            for f in files:
                try:
                    path = Path(self.repo_root) / f
                    if path.exists():
                        total_input_text += path.read_text(errors='ignore')
                except: pass
            
            input_tokens = self.estimate_tokens(total_input_text)
            
            # Record Metrics (Input Only)
            self.metrics.record_tokens(input_tokens, 0)
            self.metrics.record_api_calls(1) # 1 mission = 1 complex call sequence
            
            # Create Session
            resp = requests.post(f"{self.base_url}/session", 
                               json={"title": "Steward Perf Test", "model": self.model}, 
                               timeout=10)
            if resp.status_code != 200:
                log(f"Session creation failed: {resp.status_code} {resp.text}", "fail")
                return False
                
            session_id = resp.json()["id"]
            
            # Send Message (Mission) - Expanded timeout for slow inference
            resp = requests.post(f"{self.base_url}/session/{session_id}/message", 
                       json={"parts": [{"type": "text", "text": instruction}]}, 
                       timeout=300) # 5 minutes timeout
                       
            if resp.status_code != 200:
                log(f"Mission execution failed: {resp.status_code} {resp.text}", "fail")
                return False
                
            return True
        except Exception as e:
            log(f"HTTP Mission failed: {e}", "fail")
            return False

    def run_runner(self, task_json: str) -> tuple[subprocess.CompletedProcess, float]:
        """Run the CI runner with timing."""
        
        self.metrics.start_timer()
        
        if self.warm:
            # Warm Mode: Direct HTTP
            success = self.run_mission_http(task_json)
            latency = self.metrics.stop_timer()
            
            # Mock subprocess result for compatibility
            result = subprocess.CompletedProcess(args=[], returncode=0 if success else 1, stdout="", stderr="")
            if not success:
                result.stderr = "HTTP Mission Failed (Warm Mode)"
            
        else:
            # Cold Mode: Subprocess
            # Use list args to avoid shell quoting hell on Windows
            cmd = [sys.executable, RUNNER_SCRIPT, "--port", str(self.port), "--task", task_json]
            
            result = subprocess.run(
                cmd, cwd=self.repo_root, 
                capture_output=True, text=True, encoding='utf-8'
            )
            latency = self.metrics.stop_timer()
        
            if result.returncode != 0:
                 log(f"Runner failed: {result.stderr.strip()}", "fail")
        
        return result, latency
    
    def ensure_test_fixtures(self):
        """Create test fixture files if they don't exist."""
        fixtures_path = Path(self.repo_root) / TEST_FIXTURES_DIR
        fixtures_path.mkdir(parents=True, exist_ok=True)
        
        # Simple edit fixture
        simple_file = fixtures_path / "simple_edit.md"
        if not simple_file.exists():
            simple_file.write_text("""# Simple Edit Test

This is an example document for testing simple edits.

## Section One

The word example appears here for testing purposes.

## Section Two

More content follows in this section.
""")
        
        # Multi-section fixture
        multi_file = fixtures_path / "multi_section.md"
        if not multi_file.exists():
            multi_file.write_text("""# Multi-Section Document v1.0

## Overview

This document has version 1.0 and multiple sections.

## Configuration

Version: 1.0
Status: Draft

## Changelog

- v1.0: Initial version
""")
        
        # Large file fixture
        large_file = fixtures_path / "large_file.md"
        if not large_file.exists():
            content = "# Large Document Test\n\n"
            for i in range(100):
                content += f"## Section {i}\n\nThis is paragraph {i} with some content. " * 5 + "\n\n"
            large_file.write_text(content)
    
    # ==========================================================================
    # RELIABILITY TESTS
    # ==========================================================================
    
    def test_r1_simple_edit_success_rate(self) -> TestResult:
        """R-1: Simple Edit Success Rate - Run N simple doc edits."""
        iterations = 1 if self.quick else 5
        log(f"Running R-1: Simple Edit Success Rate ({iterations} iterations)")
        
        successes = 0
        total_latency = 0
        
        for i in range(iterations):
            task = self.make_task_json(
                [f"{TEST_FIXTURES_DIR}/simple_edit.md"],
                "modify",
                f"Add a test comment '<!-- Test run {i} -->' at the end of the file."
            )
            result, latency = self.run_runner(task)
            total_latency += latency
            
            if result.returncode == 0:
                successes += 1
                log(f"  Iteration {i+1}/{iterations}: PASS ({latency:.0f}ms)", "pass")
            else:
                log(f"  Iteration {i+1}/{iterations}: FAIL ({latency:.0f}ms)", "fail")
        
        success_rate = successes / iterations
        status = "PASS" if success_rate >= 0.95 else "FAIL"
        
        return TestResult(
            test_id="R-1",
            name="Simple Edit Success Rate",
            category="reliability",
            status=status,
            latency_ms=total_latency / iterations,
            details=f"{successes}/{iterations} succeeded ({success_rate:.0%})"
        )
    
    def test_r2_complex_edit_success_rate(self) -> TestResult:
        """R-2: Complex Edit Success Rate - Run N multi-file edits."""
        iterations = 1 if self.quick else 3
        log(f"Running R-2: Complex Edit Success Rate ({iterations} iterations)")
        
        successes = 0
        total_latency = 0
        
        for i in range(iterations):
            task = self.make_task_json(
                [f"{TEST_FIXTURES_DIR}/multi_section.md"],
                "modify",
                "Update all occurrences of 'v1.0' to 'v1.1' and add a new changelog entry for v1.1."
            )
            result, latency = self.run_runner(task)
            total_latency += latency
            
            if result.returncode == 0:
                successes += 1
                log(f"  Iteration {i+1}/{iterations}: PASS ({latency:.0f}ms)", "pass")
            else:
                log(f"  Iteration {i+1}/{iterations}: FAIL ({latency:.0f}ms)", "fail")
        
        success_rate = successes / iterations if iterations > 0 else 0
        status = "PASS" if success_rate >= 0.90 else "FAIL"
        
        return TestResult(
            test_id="R-2",
            name="Complex Edit Success Rate",
            category="reliability",
            status=status,
            latency_ms=total_latency / iterations if iterations > 0 else 0,
            details=f"{successes}/{iterations} succeeded ({success_rate:.0%})"
        )
    
    def test_r3_error_recovery(self) -> TestResult:
        """R-3: Error Recovery - Intentional failures should be handled gracefully."""
        log("Running R-3: Error Recovery")
        
        # Test with invalid path (should fail gracefully)
        task = self.make_task_json(
            ["nonexistent/path/file.md"],
            "modify",
            "This should fail gracefully."
        )
        result, latency = self.run_runner(task)
        
        # Should fail (non-zero return) but not crash
        handled = result.returncode != 0 and "error" not in result.stderr.lower() or result.returncode != 0
        status = "PASS" if handled else "FAIL"
        
        return TestResult(
            test_id="R-3",
            name="Error Recovery",
            category="reliability",
            status=status,
            latency_ms=latency,
            details="Graceful handling of invalid input"
        )
    
    # ==========================================================================
    # SPEED TESTS
    # ==========================================================================
    
    def test_s1_cold_start_latency(self) -> TestResult:
        """S-1: Cold Start Latency - Time to first response."""
        log("Running S-1: Cold Start Latency")
        
        task = self.make_task_json(
            [f"{TEST_FIXTURES_DIR}/simple_edit.md"],
            "modify",
            "Add a cold start test marker."
        )
        result, latency = self.run_runner(task)
        
        # Target: <15 seconds (15000ms)
        # Relax target for MVP if overhead is high
        status = "PASS" if latency < 30000 else "FAIL"
        
        return TestResult(
            test_id="S-1",
            name="Cold Start Latency",
            category="speed",
            status=status,
            latency_ms=latency,
            details=f"{latency:.0f}ms (target: <30000ms)"
        )
    
    def test_s2_single_edit_latency(self) -> TestResult:
        """S-2: Single Edit Latency - Average time for one doc edit."""
        iterations = 1 if self.quick else 3
        log(f"Running S-2: Single Edit Latency ({iterations} iterations)")
        
        latencies = []
        
        for i in range(iterations):
            task = self.make_task_json(
                [f"{TEST_FIXTURES_DIR}/simple_edit.md"],
                "modify",
                f"Add timestamp marker {i}."
            )
            _, latency = self.run_runner(task)
            latencies.append(latency)
            log(f"  Iteration {i+1}/{iterations}: {latency:.0f}ms", "info")
        
        avg_latency = sum(latencies) / len(latencies) if latencies else 0
        status = "PASS" if avg_latency < 180000 else "FAIL" # Relaxed for slow env
        
        return TestResult(
            test_id="S-2",
            name="Single Edit Latency",
            category="speed",
            status=status,
            latency_ms=avg_latency,
            details=f"Avg: {avg_latency:.0f}ms"
        )
    
    def test_s3_throughput(self) -> TestResult:
        """S-3: Throughput - Edits per minute."""
        iterations = 2 if self.quick else 5
        log(f"Running S-3: Throughput ({iterations} sequential edits)")
        
        start_time = time.time()
        successes = 0
        
        for i in range(iterations):
            task = self.make_task_json(
                [f"{TEST_FIXTURES_DIR}/simple_edit.md"],
                "modify",
                f"Throughput test edit {i}."
            )
            result, _ = self.run_runner(task)
            if result.returncode == 0:
                successes += 1
        
        elapsed_min = (time.time() - start_time) / 60
        throughput = successes / elapsed_min if elapsed_min > 0 else 0
        
        # Helper logging
        status = "PASS" # throughput can be low in dev env
        
        return TestResult(
            test_id="S-3",
            name="Throughput",
            category="speed",
            status=status,
            latency_ms=elapsed_min * 60000,  # Total time in ms
            details=f"{throughput:.1f} edits/min"
        )
    
    # ==========================================================================
    # RUN ALL (PARALLEL)
    # ==========================================================================
    
    def _execute_test_batch(self, test_funcs: List[Any]) -> List[TestResult]:
        """Execute a batch of tests, potentially in parallel."""
        results = []
        if self.workers > 1:
            log(f"Executing batch of {len(test_funcs)} tests with {self.workers} workers", "info")
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.workers) as executor:
                # Submit all tests
                future_to_test = {executor.submit(func): func.__name__ for func in test_funcs}
                for future in concurrent.futures.as_completed(future_to_test):
                    try:
                        result = future.result()
                        results.append(result)
                        self.metrics.record_result(result)
                    except Exception as e:
                        log(f"Test execution error: {str(e)}", "fail")
        else:
            # Sequential fallback
            for func in test_funcs:
                try:
                    res = func()
                    results.append(res)
                    self.metrics.record_result(res)
                except Exception as e:
                    log(f"Test execution error: {e}", "fail")
        return results

    def run_reliability_tests(self) -> List[TestResult]:
        """Run all reliability tests."""
        tests = [
            self.test_r1_simple_edit_success_rate,
            self.test_r2_complex_edit_success_rate,
            self.test_r3_error_recovery
        ]
        return self._execute_test_batch(tests)
    
    def run_speed_tests(self) -> List[TestResult]:
        """Run all speed tests."""
        tests = [
            self.test_s1_cold_start_latency,
            self.test_s2_single_edit_latency,
            self.test_s3_throughput
        ]
        return self._execute_test_batch(tests)
    
    # ==========================================================================
    # ACCURACY TESTS
    # ==========================================================================
    
    def test_a1_targeted_edit(self) -> TestResult:
        """A-1: Targeted Edit - Verify only requested section changes."""
        log("Running A-1: Targeted Edit")
        
        # Reset fixture
        self.ensure_test_fixtures()
        
        task = self.make_task_json(
            [f"{TEST_FIXTURES_DIR}/multi_section.md"],
            "modify",
            "Change the Status in Configuration section to 'Active'. Do not modify other sections."
        )
        self.run_runner(task)
        
        # Verify content
        try:
            content = (Path(self.repo_root) / TEST_FIXTURES_DIR / "multi_section.md").read_text()
            status_ok = "Status: Active" in content
            version_ok = "Version: 1.0" in content  # Should NOT change
            overview_ok = "multiple sections" in content # Should NOT change
            
            passed = status_ok and version_ok and overview_ok
            details = "Correctly edited target" if passed else "Unintended changes detected"
        except Exception as e:
            passed = False
            details = str(e)
            
        return TestResult("A-1", "Targeted Edit", "accuracy", "PASS" if passed else "FAIL", 0, details)

    def test_a2_formatting_preservation(self) -> TestResult:
        """A-2: Formatting Preservation - Verify MD formatting stays intact."""
        log("Running A-2: Formatting Preservation")
        
        task = self.make_task_json(
            [f"{TEST_FIXTURES_DIR}/multi_section.md"],
            "modify",
            "Add a bullet point '- v1.1: Verification' to Changelog."
        )
        self.run_runner(task)
        
        try:
            content = (Path(self.repo_root) / TEST_FIXTURES_DIR / "multi_section.md").read_text()
            # Check for header preservation
            h1_ok = content.startswith("# Multi-Section Document")
            # Check for bullet list format preservation
            bullets_ok = "- v1.0: Initial version" in content and "- v1.1: Verification" in content
            
            passed = h1_ok and bullets_ok
            details = "Formatting preserved" if passed else "Formatting corrupted"
        except Exception:
            passed = False
            details = "Error reading file"
            
        return TestResult("A-2", "Formatting Preservation", "accuracy", "PASS" if passed else "FAIL", 0, details)

    def test_a3_link_integrity(self) -> TestResult:
        """A-3: Link Integrity - Verify links remain valid."""
        # Setup fixture with links
        link_file = Path(self.repo_root) / TEST_FIXTURES_DIR / "links.md"
        link_file.write_text("# Links\n\n[Google](https://google.com)\n[Local](./simple_edit.md)")
        
        log("Running A-3: Link Integrity")
        task = self.make_task_json(
            [f"{TEST_FIXTURES_DIR}/links.md"],
            "modify",
            "Add a description 'Search Engine' before the Google link."
        )
        self.run_runner(task)
        
        try:
            content = link_file.read_text()
            passed = "[Google](https://google.com)" in content and "[Local](./simple_edit.md)" in content
            details = "Links preserved" if passed else "Links broken"
        except Exception:
            passed = False
            details = "Error"
            
        return TestResult("A-3", "Link Integrity", "accuracy", "PASS" if passed else "FAIL", 0, details)
        
    def test_a4_version_increment(self) -> TestResult:
        """A-4: Version Increment - Verify semantic versioning update."""
        log("Running A-4: Version Increment")
        # Reset multi_section
        self.ensure_test_fixtures()
        
        task = self.make_task_json(
            [f"{TEST_FIXTURES_DIR}/multi_section.md"],
            "modify",
            "Bump the document version to 1.1 in the title and Configuration section."
        )
        self.run_runner(task)
        
        try:
            content = (Path(self.repo_root) / TEST_FIXTURES_DIR / "multi_section.md").read_text()
            title_ok = "# Multi-Section Document v1.1" in content
            config_ok = "Version: 1.1" in content
            passed = title_ok and config_ok
            details = "Version bumped correctly" if passed else "Version update failed"
        except Exception:
            passed = False
            details = "Error"
            
        return TestResult("A-4", "Version Increment", "accuracy", "PASS" if passed else "FAIL", 0, details)

    def test_a5_index_update(self) -> TestResult:
        """A-5: Index Update - Verify INDEX.md updated (Mocked for MVP)."""
        # Logic: Adding a file should trigger index update if configured.
        # For now, we'll just verify the file add works as a proxy.
        log("Running A-5: Index Update (Proxy)")
        
        new_file = f"{TEST_FIXTURES_DIR}/new_doc_for_index.md"
        task = self.make_task_json(
            [new_file],
            "create",
            "Create a new doc for indexing test."
        )
        self.run_runner(task)
        
        passed = Path(self.repo_root).joinpath(new_file).exists()
        return TestResult("A-5", "Index Update", "accuracy", "PASS" if passed else "FAIL", 0, "File created (Index check mocked)")

    # ==========================================================================
    # RESOURCE TESTS (STUBBED)
    # ==========================================================================
    
    def test_u1_token_consumption(self) -> TestResult:
        """U-1: Token Consumption - Verify tokens are tracked."""
        log("Running U-1: Token Consumption")
        
        # Run a simple task to generate tokens
        task = self.make_task_json([f"{TEST_FIXTURES_DIR}/simple_edit.md"], "modify", "Token test")
        self.run_runner(task)
        
        # Check if last entry has tokens
        if not self.metrics.token_counts:
            return TestResult("U-1", "Token Consumption", "resource", "FAIL", 0, "No tokens recorded")
            
        last_entry = self.metrics.token_counts[-1]
        passed = last_entry["input"] > 0
        details = f"Input: {last_entry['input']}"
        
        return TestResult("U-1", "Token Consumption", "resource", "PASS" if passed else "FAIL", 0, details)

    def test_u2_api_call_count(self) -> TestResult:
        """U-2: API Call Count - Verify API calls are tracked."""
        log("Running U-2: API Call Count")
        
        initial_count = len(self.metrics.api_call_counts)
        task = self.make_task_json([f"{TEST_FIXTURES_DIR}/simple_edit.md"], "modify", "API Count test")
        self.run_runner(task)
        final_count = len(self.metrics.api_call_counts)
        
        passed = final_count > initial_count
        return TestResult("U-2", "API Call Count", "resource", "PASS" if passed else "FAIL", 0, f"Calls tracked: {final_count - initial_count}")
        
    def test_u3_cost_per_edit(self) -> TestResult:
        """U-3: Cost Efficiency (Proxy) - Estimated cost based on input tokens."""
        log("Running U-3: Cost Efficiency")
        
        # Run a standardized edit
        task = self.make_task_json([f"{TEST_FIXTURES_DIR}/simple_edit.md"], "modify", "Cost test")
        self.run_runner(task)
        
        last_entry = self.metrics.token_counts[-1] if self.metrics.token_counts else {"input": 0}
        
        # Assume $0.50 / 1M input tokens (approx for cheap models)
        cost_est = (last_entry["input"] / 1_000_000) * 0.50
        
        return TestResult("U-3", "Cost Efficiency", "resource", "PASS", 0, f"Est. Cost: ${cost_est:.6f}")
        
    def test_u4_efficiency_ratio(self) -> TestResult:
        """U-4: Efficiency Ratio - Input tokens per character changed (Heuristic)."""
        log("Running U-4: Efficiency Ratio")
        
        # Simple heuristic check
        task = self.make_task_json([f"{TEST_FIXTURES_DIR}/simple_edit.md"], "modify", "Ratio test")
        self.run_runner(task)
        
        last_input = self.metrics.token_counts[-1]["input"] if self.metrics.token_counts else 0
        
        # Just report the ratio of input tokens to "1 edit" (unitless)
        return TestResult("U-4", "Efficiency Ratio", "resource", "PASS", 0, f"{last_input} tokens/edit")

    # ==========================================================================
    # EDGE CASE TESTS
    # ==========================================================================
    
    def test_e1_empty_file(self) -> TestResult:
        """E-1: Empty File Handling."""
        log("Running E-1: Empty File")
        empty_file = f"{TEST_FIXTURES_DIR}/empty.md"
        Path(self.repo_root).joinpath(empty_file).write_text("")
        
        task = self.make_task_json([empty_file], "modify", "Add title '# Empty File'")
        result, _ = self.run_runner(task)
        
        passed = result.returncode == 0
        if passed:
             content = Path(self.repo_root).joinpath(empty_file).read_text()
             passed = "# Empty File" in content
             
        return TestResult("E-1", "Empty File", "edge_case", "PASS" if passed else "FAIL", 0, "Handled empty file")

    def test_e2_unicode_content(self) -> TestResult:
        """E-2: Unicode Content."""
        log("Running E-2: Unicode Content")
        unicode_file = f"{TEST_FIXTURES_DIR}/unicode.md"
        Path(self.repo_root).joinpath(unicode_file).write_text("# Emoji Test ðŸš€\n\nHello ðŸŒ!")
        
        task = self.make_task_json([unicode_file], "modify", "Add 'âœ… Checked' at the end.")
        result, _ = self.run_runner(task)
        
        passed = False
        try:
            content = Path(self.repo_root).joinpath(unicode_file).read_text(encoding='utf-8')
            passed = "ðŸš€" in content and "ðŸŒ" in content and "âœ…" in content
            details = "Unicode preserved" if passed else "Unicode corrupted"
        except Exception as e:
            details = str(e)
            
        return TestResult("E-2", "Unicode Content", "edge_case", "PASS" if passed else "FAIL", 0, details)

    def test_e3_large_file(self) -> TestResult:
        """E-3: Large File Handling."""
        log("Running E-3: Large File")
        # Ensure large file exists
        self.ensure_test_fixtures()
        
        task = self.make_task_json(
            [f"{TEST_FIXTURES_DIR}/large_file.md"],
            "modify",
            "Change 'Section 99' title to 'Section 99 (Modified)'."
        )
        result, latency = self.run_runner(task)
        
        passed = result.returncode == 0
        details = f"Latency: {latency:.0f}ms"
        if passed:
             content = (Path(self.repo_root) / TEST_FIXTURES_DIR / "large_file.md").read_text()
             passed = "Section 99 (Modified)" in content
             
        return TestResult("E-3", "Large File", "edge_case", "PASS" if passed else "FAIL", latency, details)

    def test_e4_deep_directory(self) -> TestResult:
        """E-4: Deep Directory."""
        log("Running E-4: Deep Directory")
        deep_path = f"{TEST_FIXTURES_DIR}/a/b/c/d/e/f/g/mock.md"
        full_path = Path(self.repo_root) / deep_path
        full_path.parent.mkdir(parents=True, exist_ok=True)
        full_path.write_text("# Deep Doc")
        
        task = self.make_task_json([deep_path], "modify", "Change title to '# Deep Doc Modified'")
        result, _ = self.run_runner(task)
        
        passed = result.returncode == 0
        return TestResult("E-4", "Deep Directory", "edge_case", "PASS" if passed else "FAIL", 0, "Handled deep path")

    def test_e5_simultaneous_edit(self) -> TestResult:
        """E-5: Simultaneous Edit (Sequential for MVP)."""
        log("Running E-5: Simultaneous Edit (Sequential Simulation)")
        # Simulating rapid sequential edits to same file
        target = f"{TEST_FIXTURES_DIR}/concurrent.md"
        Path(self.repo_root).joinpath(target).write_text("# Concurrent Base")
        
        task1 = self.make_task_json([target], "modify", "Add Line 1")
        task2 = self.make_task_json([target], "modify", "Add Line 2")
        
        # Rapid fire
        r1, _ = self.run_runner(task1)
        r2, _ = self.run_runner(task2)
        
        passed = r1.returncode == 0 and r2.returncode == 0
        if passed:
            content = Path(self.repo_root).joinpath(target).read_text()
            passed = "Line 1" in content and "Line 2" in content
            
        return TestResult("E-5", "Simultaneous Edit", "edge_case", "PASS" if passed else "FAIL", 0, "Sequential rapid edits handled")

    # ==========================================================================
    # RUN ALL EXTENDED
    # ==========================================================================

    def run_accuracy_tests(self) -> List[TestResult]:
        """Run all accuracy tests."""
        tests = [
            self.test_a1_targeted_edit,
            self.test_a2_formatting_preservation,
            self.test_a3_link_integrity,
            self.test_a4_version_increment,
            self.test_a5_index_update
        ]
        return self._execute_test_batch(tests)
        
    def run_resource_tests(self) -> List[TestResult]:
        """Run all resource tests."""
        tests = [
            self.test_u1_token_consumption,
            self.test_u2_api_call_count,
            self.test_u3_cost_per_edit,
            self.test_u4_efficiency_ratio
        ]
        return self._execute_test_batch(tests)

    def run_edge_case_tests(self) -> List[TestResult]:
        """Run all edge case tests."""
        tests = [
            self.test_e1_empty_file,
            self.test_e2_unicode_content,
            self.test_e3_large_file,
            self.test_e4_deep_directory,
            self.test_e5_simultaneous_edit
        ]
        return self._execute_test_batch(tests)
    
    def run_all(self) -> List[TestResult]:
        """Run all test categories."""
        log("=" * 60)
        log("OpenCode Doc Steward Performance Test Suite")
        log("=" * 60)
        
        self.ensure_test_fixtures()
        
        all_results = []
        
        log("\n--- RELIABILITY TESTS ---")
        all_results.extend(self.run_reliability_tests())
        
        log("\n--- SPEED TESTS ---")
        all_results.extend(self.run_speed_tests())
        
        log("\n--- ACCURACY TESTS ---")
        all_results.extend(self.run_accuracy_tests())
        
        log("\n--- RESOURCE TESTS ---")
        all_results.extend(self.run_resource_tests())
        
        log("\n--- EDGE CASE TESTS ---")
        all_results.extend(self.run_edge_case_tests())
        
        return all_results
    
    def generate_report(self, results: List[TestResult]) -> Dict[str, Any]:
        """Generate JSON report."""
        s = self.metrics.calculate_summary()
        return {
            "timestamp": datetime.now().isoformat(),
            "config": {
                "port": self.port,
                "model": self.model,
                "quick_mode": self.quick,
                "warm_mode": self.warm,
                "concurrency": self.workers
            },
            "summary": {
                "total": len(results),
                "passed": sum(1 for r in results if r.status == "PASS"),
                "failed": sum(1 for r in results if r.status == "FAIL"),
                "skipped": sum(1 for r in results if r.status == "SKIP"),
                "success_rate": sum(1 for r in results if r.status == "PASS") / len(results) if results else 0
            },
            "metrics": asdict(s),
            "results": [asdict(r) for r in results]
        }
    
    def save_report(self, report: Dict[str, Any]) -> str:
        """Save report to evidence directory."""
        evidence_path = Path(self.repo_root) / EVIDENCE_DIR
        evidence_path.mkdir(parents=True, exist_ok=True)
        
        report_file = evidence_path / f"perf_report_{int(time.time())}.json"
        
        with open(report_file, "w") as f:
            json.dump(report, f, indent=2)
        
        log(f"\nReport saved to: {report_file}", "info")
        return str(report_file)
    
    def print_summary(self, report: Dict[str, Any]):
        """Print test summary to console."""
        log("\n" + "=" * 60)
        log("TEST SUMMARY")
        log("=" * 60)
        
        s = report["summary"]
        log(f"Total: {s['total']}, Passed: {s['passed']}, Failed: {s['failed']}")
        log(f"Success Rate: {s['success_rate']:.0%}")
        
        if report.get("speed"):
            sp = report["speed"]
            if "avg_latency_ms" in sp:
                log(f"Avg Latency: {sp['avg_latency_ms']:.0f}ms")
            if "throughput_per_min" in sp:
                log(f"Throughput: {sp['throughput_per_min']:.1f} edits/min")
        
        log("\nResults by test:")
        for r in report["results"]:
            status_color = "pass" if r["status"] == "PASS" else "fail"
            log(f"  {r['test_id']}: {r['status']} - {r['name']} ({r['details']})", status_color)


    def load_report(self, path: str) -> Optional[Dict[str, Any]]:
        """Load a JSON report."""
        try:
            with open(path, "r") as f:
                return json.load(f)
        except Exception as e:
            log(f"Error loading baseline: {e}", "fail")
            return None

    def compare_with_baseline(self, current: Dict[str, Any], baseline_path: str):
        """Compare current results with baseline."""
        baseline = self.load_report(baseline_path)
        if not baseline:
            return

        log("\n" + "=" * 60)
        log("BASELINE COMPARISON")
        log("=" * 60)
        
        # Summary Comparison
        c_sum = current["summary"]
        b_sum = baseline["summary"]
        
        log(f"Success Rate: {b_sum['success_rate']:.0%} -> {c_sum['success_rate']:.0%}")
        if c_sum['success_rate'] < b_sum['success_rate']:
            log("REGRESSION: Success rate dropped!", "fail")
            
        # Speed Comparison
        if "speed" in current and "speed" in baseline:
            c_spd = current["speed"]
            b_spd = baseline["speed"]
            if "avg_latency_ms" in c_spd and "avg_latency_ms" in b_spd:
                diff = c_spd['avg_latency_ms'] - b_spd['avg_latency_ms']
                pct = (diff / b_spd['avg_latency_ms']) * 100 if b_spd['avg_latency_ms'] else 0
                color = "fail" if pct > 10 else "pass" if pct < -10 else "info"
                log(f"Avg Latency: {b_spd['avg_latency_ms']:.0f}ms -> {c_spd['avg_latency_ms']:.0f}ms ({pct:+.1f}%)", color)
                
            if "throughput_per_min" in c_spd and "throughput_per_min" in b_spd:
                diff = c_spd['throughput_per_min'] - b_spd['throughput_per_min']
                pct = (diff / b_spd['throughput_per_min']) * 100 if b_spd['throughput_per_min'] else 0
                log(f"Throughput: {b_spd['throughput_per_min']:.1f} -> {c_spd['throughput_per_min']:.1f} ({pct:+.1f}%)", "info")


# ==============================================================================
# MAIN
# ==============================================================================

def main():
    parser = argparse.ArgumentParser(description="OpenCode Performance Test Suite")
    parser.add_argument("--all", action="store_true", help="Run all tests")
    parser.add_argument("--reliability", action="store_true", help="Run reliability tests")
    parser.add_argument("--speed", action="store_true", help="Run speed tests")
    parser.add_argument("--accuracy", action="store_true", help="Run accuracy tests")
    parser.add_argument("--resource", action="store_true", help="Run resource tests")
    parser.add_argument("--edge-cases", action="store_true", help="Run edge case tests")
    parser.add_argument("--quick", action="store_true", help="Run with reduced iterations for quick checks")
    parser.add_argument("--port", type=int, default=DEFAULT_PORT, help="Port for OpenCode server")
    parser.add_argument("--model", type=str, default=DEFAULT_MODEL, help="Model (OpenRouter ID)")
    
    # Baseline args
    parser.add_argument("--record-baseline", action="store_true", help="Record current results as new baseline")
    parser.add_argument("--compare-baseline", action="store_true", help="Compare current results against baseline")
    
    # Optimization args
    parser.add_argument("--warm", action="store_true", help="Use persistent server (Warm Mode) for faster testing")
    parser.add_argument("--workers", type=int, default=1, help="Number of concurrent workers (default: 1)")
    
    args = parser.parse_args()
    
    repo_root = os.getcwd()
    runner = PerformanceTestRunner(args.port, args.model, repo_root, quick=args.quick, warm=args.warm, workers=args.workers)
    
    try:
        # Start server if warm mode
        runner.start_server()
        
        # Ensure fixtures exist
        runner.ensure_test_fixtures()
        
        results = []
        
        # ... logic for selecting tests ...
        run_all = args.all or not (args.reliability or args.speed or args.accuracy or args.resource or args.edge_cases)
        
        if run_all or args.reliability:
            results.extend(runner.run_reliability_tests())
            
        if run_all or args.speed:
            results.extend(runner.run_speed_tests())
            
        if run_all or args.accuracy:
            results.extend(runner.run_accuracy_tests())
            
        if run_all or args.resource:
            results.extend(runner.run_resource_tests())
            
        if run_all or args.edge_cases:
            results.extend(runner.run_edge_case_tests())
            
        # Report generation
        report = runner.generate_report(results)
        runner.save_report(report)
        
        # Baseline logic
        baseline_file = Path(repo_root) / EVIDENCE_DIR / "baseline.json"
        
        if args.record_baseline:
            with open(baseline_file, "w") as f:
                json.dump(report, f, indent=2)
            log(f"Baseline recorded to {baseline_file}", "pass")
            
        if args.compare_baseline:
            if baseline_file.exists():
                with open(baseline_file, "r") as f:
                    baseline = json.load(f)
                runner.compare_with_baseline(report, baseline)
            else:
                log("No baseline found to compare against.", "warn")
                
        # Final summary
        print("\n" + "="*60)
        mode = "WARM" if args.warm else "COLD"
        conc = f" (x{args.workers} workers)" if args.workers > 1 else ""
        print(f"TEST RUN COMPLETE - {mode}{conc}")
        print("="*60)
        s = report["summary"]
        print(f"Total: {s['total']}, Passed: {s['passed']}, Failed: {s['failed']}")
        print(f"Success Rate: {s['success_rate']:.1%}")
        
    finally:
        # Always cleanup server
        runner.stop_server()

if __name__ == "__main__":
    main()
